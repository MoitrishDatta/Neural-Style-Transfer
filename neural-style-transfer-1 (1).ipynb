{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!mkdir output","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!ls-la","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import numpy as np\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras.applications import vgg19\nfrom tensorflow.keras.applications.vgg19 import preprocess_input\nfrom tensorflow.keras.models import Model \nfrom PIL import Image\nimport matplotlib.pyplot as plt\n#img_array1=np.array(Image.open('/kaggle/input/pp.jpg'))(also works)\nimg_array1=plt.imread(\"/kaggle/input/1230000/DSC04071.JPG\")\nplt.subplot(1,2,1)\nplt.imshow(img_array1)\n#img_array2=np.array(Image.open('/kaggle/input/download.jpg'))\nimg_array2=plt.imread(\"/kaggle/input/abc-12/light_pattern.jpg\")\nplt.subplot(1,2,2)\nplt.imshow(img_array2)\nprint(img_array1.shape)\nprint(img_array2.shape)\nresult_prefix=\"mili\"\ntotal_variation_weight=1e-6\nstyle_weight=1e-6\ncontent_weight=2.5e-8\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def load_image(image):\n    image=plt.imread(image)\n    img=tf.image.convert_image_dtype(image,tf.float32)\n    img=tf.image.resize(img,[400,400])\n    img=img[tf.newaxis,:]\n    return img","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"content=load_image('/kaggle/input/neural-style-transfer/pp.jpg')\nstyle=load_image('/kaggle/input/abc-12/light_pattern.jpg')\nprint(content.shape)\nprint(style.shape)\n#plt.imshow(content.reshape(400,400,3))(doesn't work)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"vgg=tf.keras.applications.VGG19(include_top=False,weights='imagenet')\nvgg.trainable=False","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for layer in vgg.layers:\n    print(layer.name)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"content_layers=['block4_conv2']\nstyle_layers=['block1_conv1',\n              'block2_conv1',\n              'block3_conv1',\n              'block4_conv1',\n              'block5_conv1']\nnum_content_layers=len(content_layers)\nnum_style_layers=len(style_layers)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def mini_model(layer_names,model):\n    outputs=[model.get_layer(name).output for name in layer_names]\n    model=Model([vgg.input],outputs)\n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def gram_matrix(tensor):\n    temp=tensor\n    temp=tf.squeeze(temp)\n    fun=tf.reshape(temp,[temp.shape[2],temp.shape[0]*temp.shape[1]])\n    result=tf.matmul(temp,temp,transpose_b=True)\n    gram=tf.expand_dims(result,axis=0)\n    return gram","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class Custom_Style_Model(tf.keras.models.Model):\n    def __init__(self,style_layers,content_layers):\n        super(Custom_Style_Model,self).__init__()\n        self.vgg=mini_model(style_layers+content_layers,vgg)\n        self.style_layers=style_layers\n        self.content_layers=content_layers\n        self.num_style_layers=len(style_layers)\n        self.vgg.trainable=False\n    def call(self,inputs):\n        inputs=inputs*255.0\n        preprocessed_input=preprocess_input(inputs)\n        outputs=self.vgg(preprocessed_input)\n        style_outputs,content_outputs=(outputs[:self.num_style_layers],outputs[self.num_style_layers:])\n        style_outputs=[gram_matrix(style_output)\n                      for style_output in style_outputs]\n        content_dict={content_name:value\n                     for content_name,value in zip(self.content_layers,content_outputs)}\n        style_dict={style_name:value\n                   for style_name,value in zip(self.style_layers,style_outputs)}\n        return {'content':content_dict,'style':style_dict}\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"extractor=Custom_Style_Model(style_layers,content_layers)\nstyle_targets=extractor(style)['style']\ncontent_targets=extractor(content)['content']\nopt=tf.optimizers.Adam(learning_rate=0.02)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"style_weight=100\ncontent_weight=10\nstyle_weights={'block1_conv1':1.,\n               'block2_conv1':0.8,\n               'block3_conv1':0.5,\n               'block4_conv1':0.3,\n               'block5_conv1':0.1}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def total_loss(outputs):\n    style_outputs=outputs['style']\n    content_outputs=outputs['content']\n    style_loss=tf.add_n([style_weights[name]*tf.reduce_mean((style_outputs[name]-style_targets[name])**2)\n                        for name in style_outputs.keys()])\n    style_loss*=style_weight/num_style_layers\n    content_loss=tf.add_n([tf.reduce_mean((content_outputs[name]-content_targets[name])**2)\n                           for name in content_outputs.keys()])\n    content_loss*=content_weight/num_content_layers\n    loss=style_loss+content_loss\n    return loss","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"@tf.function()\ndef train_step(image):\n    with tf.GradientTape() as tape:\n        outputs=extractor(image)\n        loss=total_loss(outputs)\n    grad=tape.gradient(loss,image)\n    opt.apply_gradients([(grad,image)])\n    image.assign(tf.clip_by_value(image,clip_value_min=0.0,clip_value_max=1.0))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"target_image=tf.Variable(content)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"epochs=10\nsteps_per_epoch=100\nstep=0\nfor n in range(epochs):\n    for m in range(steps_per_epoch):\n        step+=1\n        train_step(target_image)\n    plt.imshow(np.squeeze(target_image.read_value(),0))\n    plt.title(\"Train step: {}\".format(step))\n    plt.show()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}